<!DOCTYPE html>
<html lang="en">
  <head>
     
  <link rel="stylesheet" type="text/css" href="style.css">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <!-- The line below is responsive to the screen size -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title> Project Title </title>
    <!-- Bootstrap -->
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <!-- Optional theme -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">
    <script src="https://use.fontawesome.com/87382e673b.js"></script>
      </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top" id="my-navbar">
      <div class="container">
        <div class="navbar-header">
          <a href="" class="navbar-brand">PISA Score Prediction</a>
        </div><!-- Navbar Header-->
        <div class="collapse navbar-collapse" id="navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="#Setup">Setup</a></li>
            <li><a href="#ip">Initial Approach</a></li>
            <li><a href="#np">New Approach</a></li>
            <li><a href="#Results">Results</a></li>
 			<li><a href="#ct">Consequent Testing</a></li>
 			<li><a href="#conc">Conclusion</a></li>
          </ul>
        </div><!-- End navbar body-->
      </div><!-- End container-->
    </nav><!-- End top navbar-->
    
    <div class="container">
      <div class="jumbotron">
        <div class="container text-center">
          <h2>PISA Score Prediction</h2>
          <h4><a href="http://www.cs.northwestern.edu/~ddowney/courses/349_Spring2017/" target="_blank">Northwestern University EECS 349</a> with Professor Doug Downey </a> </h4>
         </div>
        <div class="row text-center">
          <div class="col-md-4">
            <h5>Yichun Li</br>yichunli2019@u.northwestern.edu</h5> 
          </div>
          <div class="col-md-4">
            <h5>Tieyong Yu</br>tieyongyu2018@u.northwestern.edu</h5>
          </div>
          <div class="col-md-4">
            <h5>Zilun Yu</br>zilunyu2019@u.nortwhestern.edu</h5>
          </div>
        </div> 
      </div><!-- End jumbotron-->
    </div><!-- End container -->

<div class="container" id="Synopsis">
      <h4 class="text-left">Synopsis</h4>
        <div>
          PISA stands for Programme for International Student Assessment. It is a test conducted by OECD (Organisation for Economic Cooperation ande Development) in member and non-member nations of 15-year-old students annually on subjects of science, mathematics and reading. Our task is to predict the PISA test scores for 15-year-old students based on their family and school information. Our inputs include basic personal information, family educational/career background, educational/school resources, and learning experiences for each student taking the PISA exam. Our output is simply a predictive score for the critical reading section of the test for each candidate.
          <br>
          Our task is interesting because we want figure out a quantitative relationship indicating to what extent a student’s family background and school/educational resources can affect his/her academic/IQ performance. Our task is an important one because it provides us with an insight of how students perform in school. Thus, in the future, we can better identify and help the students in need.
          <br>
          In total, We tried using the following classifiers for our dataset: Decision Tree (J48), K-nearest Neighbors (IBK), Logistic Regression, Linear Regression, Multilayer Perceptron and Naive Bayes.
          <br>
          There are 23 features in total covering basic information, family background, education background and resources, and classroom experience. Through the data processing and modification of our methods, we are able to achieve prediction accuracy of around 50%. We believe that other factors not included in the official PISA set might be more indicative of the reading score than the current ones.

  </div>
</div><!-- End container-->
<div class="container" id="Final Report">
      <h4 class="text-left">Final Report</h4>
      <h5 class="text-left" id="Setup">Setup</h5>
      The dataset used is in 2009 from United national Center for Education Statistics (NCES). There are in total 5233 samples. 
      We used the following 23 features to construct the model for the dataset:
      <ul>
      	<li>Grade</li>
      	<li>Gender</li>
      	<li>Race</li>
      	<li>Place of Birth</li>
      	<li>Education level of father</li>
      	<li>Education level of mother</li>
      	<li>Whether father has full-time or part-time work</li>
      	<li>Whether mother has full-time or part-time work</li>
      	<li>Father's place of birth</li>
      	<li>Mother's place of birth</li>
      	<li>Whether father attended high school</li>
      	<li>Whether mother attended high school</li>
      	<li>Whether English is spoken at home as a primary language</li>
      	<li>Whether the student attended preschool</li>
      	<li>Whether the student expects to obtain a bachelor’s degree</li>
      	<li>Whether the student attends a public school</li>
      	<li>Whether the student’s school owns a library</li>
      	<li>The number of students in the student’s school</li>
      	<li>Whether the student has access to a computer for schoolwork</li>
      	<li>Whether the school is in the urban area</li>
      	<li>Number of minutes per week the student spends in English class</li>
      	<li>The number of students in this student’s English class at school</li>
      	<li>Whether the student reads for pleasure for 30 minutes per day</li>
      </ul>


      <h5 class="text-left" id="ip">Initial Approach</h5>
      	Originally, we used 4733 for training and 500 for testing.
      	We decided to build a linear regression model to evaluate reading test scores based on the 23 features. 
      	<br>
      	We wrote our own code to perform the lienar regression and also an error function test out the error of the model.
      	We reached a result where the final score has, on average, about 15% away from the actual score. This does not necessarily give us reliable results and insight into how students with different background and differences could be different.
      	<br>
      <h5 class="text-left" id="np">New Approach</h5>     	
      	We then decided to train and test our model based on Weka and transform the problem into a classification of letter grade of A, B, or C based on the range or scores so that we will be able to get results using different classifiers:
      	<br>
      	The student will be graded C if the score is less than 450;
      	<br>
      	The student will be graded B if the score is between 450 and 580;
      	<br>
      	The student will be graded A if the score is greater than or equal to 500.
      	<br>
      	Before we run out dataset on Weka, we also normalized all the attribute values to be between 0 and 1. Additionally, any nominal or categorical attributes are converted into binary 
      	<h6 class="text-left" id="Results">Results</h6>
      	We ran our model through Weka and we used ZeroR as our benchmark to evaluate other classifiers. Any missing value of a sample is filled with the mean value of that attribute:
      	<br>
      	<br>
      	<img src="ZeroR.png" style="width:700px;height:400px;">
      	<br>
      	<br>
      	Consequently, we ran our model on 5 classifiers to perform 10-fold cross-validation on the training set and the test set:
      	<br>
		<table class="table table-bordered">
		    <thead>
		      <tr>
		        <th>Classifier</th>
		        <th>Correctly Classified Instances(10-fold Cross-validation)</th>
		        <th>Correctly Classified Instances(supplied test set)</th>
		      </tr>
		    </thead>
		    <tbody>
		      <tr>
		        <td>Naive Bayes</td>
		        <td>50.0683%</td>
		        <td>48.9172%</td>
		      </tr>
		      <tr>
		        <td>Logistic Regression</td>
		        <td>56.8933%</td>
		        <td>53.758%</td>
		      </tr>
		      <tr>
		        <td>Multilayer Perceptron</td>
		        <td>54.1906%</td>
		        <td>50.2548%</td>
		      </tr>
		      <tr>
		        <td>K-nearest Neighbor(IBk)</td>
		        <td>45.6456%</td>
		        <td>47.1338%</td>
		      </tr>	
		      <tr>
		        <td>Decision Tree(J48)</td>
		        <td>50.2867%</td>
		        <td>50.828%</td>
		      </tr>		      	      
		    </tbody>
		</table>

      	<h6 class="text-left" id="ct">Consequent Testing</h6>
      	In addition, we also did additional data processing.
      	<ul>
      	 <li>We tried to do feature selection, where we deleted some features, in our case whether father and mother went to high school. 
      	 <br>
      	 Similarly, we did not achieve any apparent results.
      	 </li>
      	 <li> We also tried to handle missing attributes differently. Rather than using median for any value of a sample that is 'NA', we decided to delete any sample that has attributes of missing values.
      	 <br>
      	 This handling skewed our model because the samples with missing values are concentrated around those with low reading scores. Therefore, the general distribution was heavily affected and the results were thusly compromised. Here are the results:
		<table class="table table-bordered">
		    <thead>
		      <tr>
		        <th>Classifier</th>
		        <th>Correctly Classified Instances(10-fold Cross-validation)</th>
		        <th>Correctly Classified Instances(supplied test set)</th>
		      </tr>
		    </thead>
		    <tbody>
		      <tr>
		        <td>Naive Bayes</td>
		        <td>47.3488%</td>
		        <td>47.9798%</td>
		      </tr>
		      <tr>
		        <td>Logistic Regression</td>
		        <td>50.2017%</td>
		        <td>44.8485%</td>
		      </tr>
		      <tr>
		        <td>Multilayer Perceptron</td>
		        <td>45.7746%</td>
		        <td>46.3636%</td>
		      </tr>
		      <tr>
		        <td>K-nearest Neighbor(IBk)</td>
		        <td>42.6263%</td>
		        <td>41.9192%</td>
		      </tr>	
		      <tr>
		        <td>Decision Tree(J48)</td>
		        <td>46.5617%</td>
		        <td>45.7576%</td>
		      </tr>		      	      
		    </tbody>
		</table>
      	 We then tried to replace the 'NA' values with mean of each attribute and that did not achieve any apparent improvement in the model either.
      	</ul>
      <h5 class="text-left" id="conc">Conclusion</h5>
      	In conclusion, we do find that there is a weak relationship between all the attributes provided in the dataset and the final reading score despit the fact that qualitatively, we would assume a strong correlation. Performing feature selection could also be potentially a good approach into reducing the effect of overfitting and irrelevant variables.
      	<br>
      	Additionally, we do think that using potentially other attributes could be beneficial to finding the correlation between the student's background and their reading score performances. Attributes such as nationality, the students' score in science and mathematics, as well as the difficulty of the curriculum could potentially play a more direct role in predicting the student's reading test score. 
      	<br>
		<ul class="nav navbar-nav">
		<li><a href="RawData/" download target="_blank"> <center> <figure>  <img src="rawdata.png" style="width:35px" </img> <figcaption>Raw Feature Data (.CSV) </figcaption> </figure> </center></a></li>
		     <li><a href="https://yichunli95.github.io/EECS349/"  target="_blank"> <center> <figure>  <img src="github.png" style="width:35px" </img> <figcaption>Project GitHub </figcaption> </figure> </center></a></li>                
		</ul>
 </div>


 
    <script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
  </body>
</html>
